---
category: Article
article: Article 13  Transparency and provision of information to users.md
---

1. [High-risk AI systems](High-risk AI systems.html) shall be designed and developed in such a way to ensure that their operation is sufficiently transparent to enable users to interpret the system’s output and use it appropriately. An appropriate type and degree of transparency shall be ensured, with a view to achieving compliance with the relevant obligations of the [user](user.html) and of the [provider](provider.html) set out in Chapter 3 of this Title.

2. [High-risk AI systems](High-risk AI systems.html) shall be accompanied by [instructions for use](instructions for use.html) in an appropriate digital format or otherwise that include concise, complete, correct and clear information that is relevant, accessible and comprehensible to users.

3. The information referred to in paragraph 2 shall specify:

	1. {a}(a)the identity and the contact details of the [provider](provider.html) and, where applicable, of its [authorised representative](authorised representative.html);
	2. (b)the characteristics, capabilities and limitations of performance of the high-risk AI system, including:
		1. (i)its [intended purpose](intended purpose.html);
		2. (ii)the level of accuracy, robustness and cybersecurity referred to in Article 15 against which the high-risk AI system has been tested and validated and which can be expected, and any known and foreseeable circumstances that may have an impact on that expected level of accuracy, robustness and cybersecurity;
		3. (iii)any known or foreseeable circumstance, related to the use of the high-risk AI system in accordance with its [intended purpose](intended purpose.html) or under conditions of [reasonably foreseeable misuse](reasonably foreseeable misuse.html), which may lead to risks to the health and safety or fundamental rights;
		4. (iv)its performance as regards the persons or groups of persons on which the system is intended to be used;		5. (v)when appropriate, specifications for the [[input data]([[input data.html)|[input da](input da.html)ta|[input da](input da.html)ta|[input da](input da.html)ta]([input data]([[input data.html)|[input da](input da.html)ta|[input da](input da.html)ta|[input da](input da.html)ta.html), or any other relevant info[testing data|rmation in t](testing data|rmation in t.html)erms of the training, validation and [testing data](testing data.html) sets used, taking into account the [intended purpose](intended purpose.html) of the AI system.
	3. (c)the changes to the high-risk AI system and its performance which have been pre-determined by the [provider](provider.html) at the moment of the initial [conformity assessment](conformity assessment.html), if any; 
	4. (d)the human oversight measures referred to in Article 14, including the technical measures put in place to facilitate the interpretation of the outputs of AI systems by the users;
	5. (e)the expected lifetime of the high-risk AI system and any necessary maintenance and care measures to ensure the proper functioning of that AI system, including as regards software updates.