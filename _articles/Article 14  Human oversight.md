---
category: Article
article: Article 14  Human oversight.md
---

1. [High-risk AI systems](High-risk AI systems.html) shall be designed and developed in such a way, including with appropriate human-machine interface tools, that they can be effectively overseen by natural persons during the period in which the AI system is in use.

2. Human oversight shall aim at preventing or minimising the risks to health, safety or fundamental rights that may emerge when a high-risk AI system is used in accordance with its [intended purpose](intended purpose.html) or under conditions of [reasonably foreseeable misuse](reasonably foreseeable misuse.html), in particular when such risks persist notwithstanding the application of other requirements set out in this Chapter.

3. Human oversight shall be ensured through either one or all of the following measures:

1. {a}(a)identified and built, when technically feasible, into the high-risk AI system by the [provider](provider.html) before it is placed on the market or put into service;
2. (b)identified by the [provider](provider.html) before placing the high-risk AI system on the market or putting it into service and that are appropriate to be implemented by the [user](user.html).

4. The measures referred to in paragraph 3 shall enable the individuals to whom human oversight is assigned to do the following, as appropriate to the circumstances:

	1. {a}(a)fully understand the capacities and limitations of the high-risk AI system and be able to duly monitor its operation, so that signs of anomalies, dysfunctions and unexpected performance can be detected and addressed as soon as possible
	2. (b)remain aware of the possible tendency of automatically relying or over-relying on the output produced by a high-risk AI system (‘automation bias’), in particular for [high-risk AI systems](high-risk AI systems.html) used to provide information or recommendations for decisions to be taken by natural persons;
	3. (c)be able to correctly interpret the high-risk AI system’s output, taking into account in particular the characteristics of the system and the interpretation tools and methods available;
	4. (d)be able to decide, in any particular situation, not to use the high-risk AI system or otherwise disregard, override or reverse the output of the high-risk AI system;
	5. (e)be able to intervene on the operation of the high-risk AI system or interrupt the system through a “stop” button or a similar procedure.

5. For [high-risk AI systems](high-risk AI systems.html) referred to in point 1(a) of Annex III, the measures referred to in paragraph 3 shall be such as to ensure that, in addition, no action or decision is taken by the [user](user.html) on the basis of the identification resulting from the system unless this has been verified and confirmed by at least two natural persons.